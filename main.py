import os
import time
import requests
import hashlib
import re
import unicodedata
import html
import xml.etree.ElementTree as ET
import gc
from contextlib import asynccontextmanager
from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv
from openai import OpenAI
from pinecone import Pinecone
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from typing import List, Optional, Dict, Any

# --- KONFIGUR√ÅCI√ì ---
load_dotenv()
INDEX_NAME = "booksy-index"
XML_FEED_URL = os.getenv("XML_FEED_URL", "https://www.antikvarius.ro/wp-content/uploads/woo-feed/google/xml/booksyfullfeed.xml")
TEMP_FILE = "temp_feed.xml"

# --- TUD√ÅSB√ÅZIS (RO LINKS - FIX) ---
POLICY_PAGES = [
    {"url": "https://www.antikvarius.ro/termeni-si-conditii-de-utilizare/", "lang": "ro", "name": "Termeni »ôi condi»õii"},
    {"url": "https://www.antikvarius.ro/informatii-despre-plata/", "lang": "ro", "name": "Informa»õii despre platƒÉ"},
    {"url": "https://www.antikvarius.ro/informatii-despre-livrare/", "lang": "ro", "name": "Informa»õii despre livrare"},
    {"url": "https://www.antikvarius.ro/contact/", "lang": "ro", "name": "Contact"},
]

# --- ADATMODELLEK ---
class ChatRequest(BaseModel):
    message: str
    context_url: Optional[str] = "" 

# --- HELPEREK ---
def normalize_text(text):
    if not text: return ""
    text = str(text).lower()
    return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')

def safe_str(val):
    if val is None: return ""
    return html.unescape(str(val).strip())

def clean_html_structural(raw_html):
    if not raw_html: return ""
    s = safe_str(raw_html)
    s = s.replace('</div>', '\n').replace('</p>', '\n').replace('<br>', '\n').replace('<br/>', '\n')
    cleanr = re.compile('<.*?>')
    cleantext = re.sub(cleanr, ' ', s)
    cleantext = cleantext.replace("<![CDATA[", "").replace("]]>", "")
    cleantext = re.sub(r'<script.*?>.*?</script>', '', cleantext, flags=re.DOTALL)
    cleantext = re.sub(r'<style.*?>.*?</style>', '', cleantext, flags=re.DOTALL)
    return "\n".join([line.strip() for line in cleantext.split('\n') if line.strip()])

def extract_all_data(elem) -> Dict[str, Any]:
    data = {}
    for child in elem:
        tag = child.tag.split('}')[-1].lower()
        val = safe_str(child.text)
        if val: data[tag] = val
    return data

def generate_content_hash(data_string):
    return hashlib.md5(data_string.encode('utf-8')).hexdigest()

def detect_hungarian_intent(msg):
    hu_words = ["szia", "k√∂nyv", "keres", "hogy", "mi√©rt", "mennyi", "sz√°ll√≠t√°s", "fizet√©s", "van", "nincs", "mikor", "kiad√≥", "szerz≈ë", "c√≠m", "magyar"]
    msg_norm = normalize_text(msg)
    if any(w in msg_norm for w in hu_words): return True
    return False

# --- AUTOMATIZ√ÅLT FRISS√çT≈ê MOTOR (V64 - CATEGORY OVERRIDE) ---
class AutoUpdater:
    def __init__(self):
        self.api_key_openai = os.getenv("OPENAI_API_KEY")
        self.api_key_pinecone = os.getenv("PINECONE_API_KEY")
        self.client_ai = OpenAI(api_key=self.api_key_openai)
        self.pc = Pinecone(api_key=self.api_key_pinecone)
        self.index = self.pc.Index(INDEX_NAME)

    def download_feed(self):
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
        for attempt in range(3):
            try:
                print(f"‚¨áÔ∏è [DOWNLOAD] XML Feed Let√∂lt√©s (K√≠s√©rlet {attempt+1}/3)...")
                with requests.get(XML_FEED_URL, headers=headers, stream=True, timeout=300) as r:
                    r.raise_for_status()
                    with open(TEMP_FILE, 'wb') as f:
                        for chunk in r.iter_content(chunk_size=8192):
                            f.write(chunk)
                file_size = os.path.getsize(TEMP_FILE)
                if file_size < 10000: raise Exception("T√∫l kicsi f√°jl.")
                print(f"‚úÖ [DOWNLOAD] Siker! M√©ret: {file_size / 1024 / 1024:.2f} MB")
                return True
            except Exception as e:
                print(f"‚ö†Ô∏è Hiba: {e}")
                time.sleep(5)
        return False

    def update_policies(self, current_ts):
        print("‚ÑπÔ∏è [POLICY] Rom√°n inform√°ci√≥s oldalak friss√≠t√©se...")
        headers = {'User-Agent': 'BooksyBot/1.0'}
        for page in POLICY_PAGES:
            try:
                url = page['url']
                r = requests.get(url, headers=headers, timeout=30)
                if r.status_code == 200:
                    raw_html = r.text
                    clean_text = clean_html_structural(raw_html)
                    if len(clean_text) > 20000: clean_text = clean_text[:20000]

                    d_hash = generate_content_hash(clean_text)
                    page_id = f"policy_{generate_content_hash(url)}"
                    
                    emb_text = f"T√≠pus: Szab√°lyzat (ro). C√≠m: {page['name']}. Tartalom: {clean_text[:8000]}"
                    emb = self.client_ai.embeddings.create(input=emb_text, model="text-embedding-3-small").data[0].embedding
                    
                    meta = {
                        "title": page['name'], "url": url, "text": clean_text[:25000],
                        "lang": "ro", "type": "policy", "content_hash": d_hash, "last_seen": current_ts
                    }
                    self.index.upsert(vectors=[(page_id, emb, meta)])
                    print(f"   ‚úÖ [POLICY] OK: {page['name']}")
                else: print(f"   ‚ö†Ô∏è Hiba: {r.status_code} - {url}")
            except Exception as e: print(f"   ‚ùå Hiba: {e}")

    def run_daily_update(self):
        print(f"üîÑ [AUTO] Napi Friss√≠t√©s Ind√≠t√°sa (V64)")
        current_sync_ts = int(time.time())
        
        # 1. Policy
        self.update_policies(current_sync_ts)
        
        # 2. K√∂nyvek
        if not self.download_feed(): return

        try:
            print("üöÄ [MODE] Parsing Books from Disk")
            context = ET.iterparse(TEMP_FILE, events=("end",))
            batch = []
            count_total = 0
            count_updated = 0
            count_skipped = 0
            
            for event, elem in context:
                tag_local = elem.tag.split('}')[-1].lower()
                if tag_local in ['item', 'post']:
                    count_total += 1
                    try:
                        item_data = extract_all_data(elem)
                        bid = item_data.get('id') or item_data.get('post_id') or item_data.get('g:id')
                        
                        if bid:
                            title = item_data.get('title') or "Nincs c√≠m"
                            desc = item_data.get('description', '')
                            short_desc = item_data.get('shortdescription', '') or item_data.get('excerpt', '')
                            full_raw_text = f"{desc} {short_desc}"
                            clean_desc = clean_html_structural(full_raw_text)
                            
                            category = item_data.get('product_type') or item_data.get('category') or ""
                            cat_norm = normalize_text(category)
                            
                            # --- 1. PUBLISHER Logic (V64 FIX) ---
                            # Ha a kateg√≥ria nev√©ben benne van a "Bookman", akkor a kiad√≥ fixen Bookman!
                            pub = "Ismeretlen"
                            if "bookman" in cat_norm:
                                pub = "Bookman Kiad√≥"
                            else:
                                # Ha nincs a kateg√≥ri√°ban, keress√ºk a sz√∂vegben regex-szel
                                match_pub = re.search(r'(Kiad√≥|Kiad√°s|Publisher)(?:\s|<[^>]+>)*:?(?:\s|<[^>]+>)+([^<\n\r]+)', full_raw_text, re.IGNORECASE)
                                if match_pub:
                                    extracted = match_pub.group(2).strip()
                                    if "bookman" in extracted.lower(): pub = "Bookman Kiad√≥"
                                    else: pub = extracted
                            
                            match_auth = re.search(r'(Szerz≈ë|√çrta|Author|Szerz≈ëk)(?:\s|<[^>]+>)*:?(?:\s|<[^>]+>)+([^<\n\r]+)', full_raw_text, re.IGNORECASE)
                            auth = match_auth.group(2).strip() if match_auth else "Ismeretlen"

                            # --- 2. LANG Logic (URL alap√∫ gy√∂k√©rkateg√≥ria) ---
                            detected_lang = "hu"
                            if "carti in limba romana" in cat_norm: detected_lang = "ro"
                            elif "magyar nyelvu konyvek" in cat_norm: detected_lang = "hu"
                            
                            price = item_data.get('sale_price') or item_data.get('price') or "0"
                            
                            # Hash
                            hash_input = "".join([f"{k}:{v}" for k, v in sorted(item_data.items())])
                            hash_input += f"|{detected_lang}|{pub}|{auth}"
                            d_hash = generate_content_hash(hash_input)
                            
                            need_emb = True
                            try:
                                fetch_res = self.index.fetch(ids=[bid])
                                if fetch_res and 'vectors' in fetch_res and bid in fetch_res['vectors']:
                                    existing_meta = fetch_res['vectors'][bid]['metadata']
                                    if existing_meta.get('content_hash') == d_hash:
                                        need_emb = False
                                        count_skipped += 1
                            except: pass
                            
                            if need_emb:
                                if count_total % 500 == 0: print(f"‚è≥ [PROG] {count_total}... (Upd: {count_updated})")
                                emb_text = f"Nyelv: {detected_lang}. C√≠m: {title}. Szerz≈ë: {auth}. Kateg√≥ria: {category}. Kiad√≥: {pub}. Le√≠r√°s: {clean_desc[:800]}"
                                emb = self.client_ai.embeddings.create(input=emb_text[:8000], model="text-embedding-3-small").data[0].embedding
                                meta = {
                                    "title": title, "url": item_data.get('link', ''), "image_url": item_data.get('image_link', ''),
                                    "price": price, "publisher": pub, "author": auth, "category": category,
                                    "stock": "instock", "lang": detected_lang, "content_hash": d_hash,
                                    "last_seen": current_sync_ts, "type": "book"
                                }
                                for k, v in item_data.items():
                                    if k not in meta:
                                        clean_v = clean_html_structural(str(v))
                                        if len(clean_v) > 1000: clean_v = clean_v[:1000]
                                        meta[k] = clean_v
                                batch.append((bid, emb, meta))
                                count_updated += 1

                    except Exception as e: pass
                    elem.clear()
                    if count_total % 500 == 0: gc.collect()
                    if len(batch) >= 50:
                        self.index.upsert(vectors=batch)
                        batch = []

            if batch: self.index.upsert(vectors=batch)
            print("üßπ [AUTO] Takar√≠t√°s...")
            one_week_ago = current_sync_ts - (7 * 24 * 60 * 60)
            try: self.index.delete(filter={"last_seen": {"$lt": one_week_ago}, "type": "book"})
            except: pass
            if os.path.exists(TEMP_FILE): os.remove(TEMP_FILE)
            print(f"üèÅ [V√âGE] √ñsszes: {count_total}, Friss√≠tve: {count_updated}, Skip: {count_skipped}")

        except Exception as e: print(f"‚ùå Hiba: {e}")

# --- BRAIN (V64) ---
class BooksyBrain:
    def __init__(self):
        self.updater = AutoUpdater()
        self.client_ai = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.index = Pinecone(api_key=os.getenv("PINECONE_API_KEY")).Index(INDEX_NAME)

    def search(self, q, search_lang_filter):
        try:
            q_norm = normalize_text(q)
            results = []
            
            # 1. POLICY
            policy_keywords = ["szallitas", "fizetes", "visszakuldes", "garancia", "kapcsolat", "bolt", "cim", "telefon", "email", "nyitva", "livrare", "plata", "contact"]
            if any(k in q_norm for k in policy_keywords):
                vec = self.client_ai.embeddings.create(input=q, model="text-embedding-3-small").data[0].embedding
                return self.index.query(vector=vec, top_k=3, include_metadata=True, filter={"type": "policy"})['matches']

            # 2. BOOKMAN FILTER
            if "bookman" in q_norm:
                vec = self.client_ai.embeddings.create(input=q, model="text-embedding-3-small").data[0].embedding
                direct_res = self.index.query(vector=vec, top_k=20, include_metadata=True, filter={"publisher": "Bookman Kiad√≥", "stock": "instock"})
                for m in direct_res['matches']:
                    m['custom_score'] = 10000 
                    results.append(m)
            
            # 3. NORM√ÅL
            vec = self.client_ai.embeddings.create(input=q, model="text-embedding-3-small").data[0].embedding
            filt = {"stock": "instock", "type": "book"}
            if search_lang_filter != 'all': filt["lang"] = search_lang_filter
            normal_res = self.index.query(vector=vec, top_k=60, include_metadata=True, filter=filt)
            
            for m in normal_res['matches']:
                if any(r['id'] == m['id'] for r in results): continue
                meta = m['metadata']
                score = m['score'] * 100 
                
                if q_norm in normalize_text(meta.get('title', '')): score += 1000
                if q_norm in normalize_text(meta.get('author', '')): score += 600
                if q_norm in normalize_text(meta.get('category', '')): score += 400
                if q_norm in normalize_text(meta.get('publisher', '')): score += 300
                if q_norm in normalize_text(meta.get('description', '')): score += 100
                
                m['custom_score'] = score
                results.append(m)
            
            results.sort(key=lambda x: x['custom_score'], reverse=True)
            return results[:10]
        except: return []

    def process(self, msg, context_url=""):
        # 1. SITE CONTEXT (A TE PONTOS SZAB√ÅLYOD ALAPJ√ÅN)
        site_lang = 'ro' # Alap√©rtelmezett (https://www.antikvarius.ro/)
        if context_url and '/hu/' in str(context_url).lower(): # Ha van benne /hu/
            site_lang = 'hu'
        
        # 2. INTENT OVERRIDE
        if detect_hungarian_intent(msg):
            site_lang = 'hu'
        
        matches = self.search(msg, site_lang)
        
        prods = []
        ctx_text = ""
        is_policy = matches and matches[0]['metadata'].get('type') == 'policy'
        
        if not matches:
             err_msg = "Sajnos nem tal√°ltam k√∂nyvet." if site_lang == 'hu' else "Nu am gƒÉsit nimic."
             return {"reply": err_msg, "products": []}

        for m in matches:
            meta = m['metadata']
            if is_policy:
                ctx_text += f"--- POLICY (Nyelv: {meta.get('lang')}) ---\n{meta.get('text', '')}\n"
            else:
                details = f"C√≠m: {meta.get('title')}, √År: {meta.get('price')}, Kiad√≥: {meta.get('publisher')}, Kateg√≥ria: {meta.get('category')}"
                ctx_text += f"--- K√ñNYV ---\n{details}\n"
                p = {"title": meta.get('title'), "price": meta.get('price'), "url": meta.get('url'), "image": meta.get('image_url')}
                prods.append(p)
                if len(prods)>=8: break
            
        if site_lang == 'hu':
            sys_prompt = f"""Te a Booksy vagy, az Antikvarius.ro asszisztense.
            K√©rd√©s: "{msg}"
            ADATOK: {ctx_text}
            
            UTAS√çT√ÅS:
            1. V√°laszolj kedvesen √©s r√∂viden.
            2. NE HASZN√ÅLJ K√âPET/LINKET A SZ√ñVEGBEN!
            3. Policy info: Rom√°nul van, FORD√çTSD LE MAGYARRA.
            4. Ha Bookman kiad√≥, emeld ki.
            V√°lasz nyelve: MAGYAR."""
        else:
            sys_prompt = f"""E»ôti Booksy. Date: {ctx_text}
            Instructiuni:
            1. RƒÉspunde scurt »ôi politicos.
            2. NU include imagini/link-uri √Æn text.
            3. RƒÉspunde √Æn rom√¢nƒÉ."""

        try:
            ans = self.client_ai.chat.completions.create(
                model="gpt-4o-mini", messages=[{"role":"user", "content":sys_prompt}], temperature=0.3
            ).choices[0].message.content
        except: ans = "Hiba."
        
        return {"reply": ans, "products": prods}

# --- APP ---
bot = BooksyBrain()
scheduler = BackgroundScheduler()
scheduler.add_job(bot.updater.run_daily_update, CronTrigger(hour=3, minute=0))

@asynccontextmanager
async def lifespan(app: FastAPI):
    scheduler.start()
    yield
    scheduler.shutdown()

app = FastAPI(lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

@app.get("/")
def home(): return {"status": "Booksy V64 (CATEGORY BOOKMAN FIX)"}

@app.post("/chat")
def chat(req: ChatRequest): return bot.process(req.message, req.context_url)

@app.post("/force-update")
def force(bt: BackgroundTasks):
    bt.add_task(bot.updater.run_daily_update)
    return {"status": "V64 Update Running"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)