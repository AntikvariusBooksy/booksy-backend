import os
import difflib
import unicodedata
import re
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv
from openai import OpenAI
from pinecone import Pinecone

# --- KONFIGUR√ÅCI√ì ---
load_dotenv()
INDEX_NAME = "booksy-index"

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    message: str

class HookRequest(BaseModel):
    url: str
    page_title: str
    visitor_type: str 
    cart_status: str 
    lang: str

# --- SEG√âDF√úGGV√âNYEK ---
def normalize_text(text):
    if not text: return ""
    text = str(text).lower()
    return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')

class BooksyBrain:
    def __init__(self):
        self.api_key_openai = os.getenv("OPENAI_API_KEY")
        self.api_key_pinecone = os.getenv("PINECONE_API_KEY")
        self.client_ai = OpenAI(api_key=self.api_key_openai)
        self.pc = Pinecone(api_key=self.api_key_pinecone)
        self.index = self.pc.Index(INDEX_NAME)

        self.store_policy = """
        [SZ√ÅLL√çT√ÅS: Feldolgoz√°s (2-4 nap rakt√°r / 7-30 nap k√ºls≈ë) + Fut√°r (24-48h RO, 2-4 nap HU).]
        """

    def generate_sales_hook(self, ctx: HookRequest):
        try:
            response = self.client_ai.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": f"Context: Lang {ctx.lang}, Page {ctx.page_title}, Cart {ctx.cart_status}. Generate short sales hook (max 6 words)."},
                    {"role": "user", "content": "Hook me."}
                ],
                temperature=0.7, max_tokens=30
            )
            return response.choices[0].message.content.strip()
        except:
            return "BunƒÉ! Te pot ajuta?" if ctx.lang == 'ro' else "Szia! Seg√≠thetek?"

    def generate_search_params(self, user_input):
        try:
            response = self.client_ai.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": """
                     Analyze user query.
                     1. Language (hu/ro).
                     2. Intent (SEARCH/INFO).
                     3. Scope (ALL/SPECIFIC).
                     4. KEYWORDS: Keep names intact!
                     Output: LANG | SCOPE | INTENT | KEYWORDS
                     """},
                    {"role": "user", "content": user_input}
                ],
                temperature=0.1
            )
            parts = response.choices[0].message.content.split('|')
            return parts[0].strip().lower(), parts[1].strip(), parts[2].strip(), parts[3].strip()
        except:
            return "hu", "SPECIFIC", "SEARCH", user_input

    def search_books(self, query_text, lang_filter, scope):
        try:
            # 1. Pinecone keres√©s (Nagy mer√≠t√©s)
            response = self.client_ai.embeddings.create(input=query_text, model="text-embedding-3-small")
            
            # --- SZ≈∞R≈ê VISSZAT√âVE (CSAK RAKT√ÅRON L√âV≈êK) ---
            filter_criteria = {"stock": "instock"}
            
            if scope != 'ALL' and lang_filter in ['hu', 'ro']:
                filter_criteria["lang"] = lang_filter
            
            raw_results = self.index.query(
                vector=response.data[0].embedding,
                top_k=100, 
                include_metadata=True, 
                filter=filter_criteria
            )

            matches = raw_results.get('matches', [])
            if not matches: return {"matches": []}

            # --- 2. V15 OKOS SZ≈∞R√âS (Smart Filter) ---
            # Nem dobjuk el, ha nem "T√∂k√©letes", hanem megtartjuk, ha "Er≈ës a gyan√∫" (magas Score)
            
            stop_words = ['konyv', 'konyvek', 'konyvet', 'carte', 'carti', 'keresek', 'kiado', 'szerzo', 'cim']
            normalized_query = normalize_text(query_text)
            search_keywords = [w for w in normalized_query.split() if w not in stop_words and len(w) > 2]

            final_results = []
            seen_ids = set()

            for match in matches:
                meta = match['metadata']
                score = match['score']
                
                # C√≠m + Szerz≈ë + Kateg√≥ria sz√∂veges vizsg√°lata
                full_text_search = normalize_text(str(meta.get('title', ''))) + " " + \
                                   normalize_text(str(meta.get('author', ''))) + " " + \
                                   normalize_text(str(meta.get('category', '')))
                
                # A) T√ñK√âLETES TAL√ÅLAT (Sz√∂veges egyez√©s)
                # Pl: "Berente" benne van a c√≠mben/szerz≈ëben
                is_text_match = False
                if search_keywords:
                    match_count = 0
                    for kw in search_keywords:
                        if kw in full_text_search:
                            match_count += 1
                    # Ha a kulcsszavak fele benne van (vagy legal√°bb 1)
                    if match_count >= 1: 
                        is_text_match = True

                # B) SZEMANTIKUS TAL√ÅLAT (Vector Score)
                # Ha a sz√∂vegben nincs benne (pl. "A teny√©relemz√©s..." c√≠m), de a le√≠r√°sban igen,
                # akkor a Vector Score magas lesz. Ezt is megtartjuk!
                is_high_score = score > 0.55  # Ha el√©g er≈ës a gyan√∫

                # D√ñNT√âS: Megtartjuk, ha sz√∂veges VAGY er≈ës szemantikus tal√°lat
                if is_text_match or is_high_score:
                    if match['id'] not in seen_ids:
                        final_results.append(match)
                        seen_ids.add(match['id'])

            # Rendez√©s pontsz√°m szerint
            final_results.sort(key=lambda x: x['score'], reverse=True)
            
            return {"matches": final_results[:25]}

        except Exception as e:
            print(f"Keres√©si hiba: {e}")
            return {"matches": []}

    def process_message(self, user_input):
        detected_lang, scope, intent, keywords = self.generate_search_params(user_input)
        context_text = ""
        found_products = [] 
        
        footer_hu = "\n\nüí° *Tipp: Jelenleg a nyelvednek megfelel≈ë k√∂nyveket keresem. Ha mindent l√°tni szeretn√©l, √≠rd hozz√°: ‚Äûminden nyelven‚Äù!*"
        footer_ro = "\n\nüí° *Sfat: Caut cƒÉr»õi √Æn limba ta. DacƒÉ vrei sƒÉ vezi toate limbile, adaugƒÉ: ‚Äûtoate limbile‚Äù!*"

        if intent == "SEARCH":
            results = self.search_books(keywords, detected_lang, scope)
            seen_titles = []
            
            if not results.get('matches'):
                msg = "Nu am gƒÉsit rezultate (√Æn stoc)." if detected_lang == 'ro' else "Sajnos nem tal√°ltam k√©szleten l√©v≈ë k√∂nyvet."
                return {"reply": msg + (footer_ro if detected_lang == 'ro' else footer_hu), "products": []}

            for match in results['matches']:
                meta = match['metadata']
                title = str(meta.get('title', 'N/A'))
                
                is_dup = False
                for seen in seen_titles:
                    if difflib.SequenceMatcher(None, title.lower(), seen.lower()).ratio() > 0.85:
                        is_dup = True; break
                if is_dup: continue
                seen_titles.append(title)
                
                product_data = {
                    "title": title,
                    "price": meta.get('price', 'N/A'), 
                    "url": meta.get('url', '#'),
                    "image": meta.get('image_url', '') 
                }
                found_products.append(product_data)
                
                author = meta.get('author', 'N/A')
                cat_tag = meta.get('category', 'N/A')
                
                context_text += f"- {title} (Szerz≈ë: {author}, √År: {meta.get('price')} RON, Kateg√≥ria: {cat_tag})\n"
                
                if len(found_products) >= 8: break 
            
            if not found_products:
                msg = "Nu am gƒÉsit nimic relevant." if detected_lang == 'ro' else "Sajnos nem tal√°ltam relev√°ns k√∂nyvet."
                return {"reply": msg + (footer_ro if detected_lang == 'ro' else footer_hu), "products": []}

        else:
            context_text = "HASZN√ÅLD A TUD√ÅSB√ÅZIST!"

        lang_instruction = "Reply in ROMANIAN only." if detected_lang == 'ro' else "Reply in HUNGARIAN only."
        system_prompt = f"Te Booksy vagy. {self.store_policy} Csak a felsorolt k√∂nyvekr≈ël besz√©lj, amik rakt√°ron vannak."

        response = self.client_ai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "system", "content": lang_instruction},
                {"role": "user", "content": f"User: {user_input}\nFound Instock Books:\n{context_text}"}
            ],
            temperature=0.3
        )
        
        final_reply = response.choices[0].message.content
        if scope != 'ALL': final_reply += footer_ro if detected_lang == 'ro' else footer_hu
        return {"reply": final_reply, "products": found_products}

bot = BooksyBrain()

@app.get("/")
def home(): return {"status": "Booksy V15 (Instock ONLY + Smart Hybrid Search)"}

@app.post("/hook")
def hook_endpoint(request: HookRequest):
    return {"hook": bot.generate_sales_hook(request)}

@app.post("/chat")
def chat_endpoint(request: ChatRequest):
    return bot.process_message(request.message)